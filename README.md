# PRODIGY_GA_01

ðŸ“˜ Prodigy Infotech Internship â€“ Generative AI Task 1


ðŸ§  Task Title: Fine-Tuning GPT-2 for Text Generation

This repository contains the implementation of Task 1 from the Generative AI internship at Prodigy Infotech. The objective of this task was to fine-tune the GPT-2 model to generate contextually relevant and coherent text based on a small custom dataset.

ðŸ”§ Technologies Used

Python
Hugging Face Transformers
Google Colab
PyTorch

ðŸ“‚ Files

Task1_GPT2_Text_Generation.ipynb â€“ Colab notebook for fine-tuning GPT-2 and generating text.
data/train.txt â€“ Custom dataset used for training.

ðŸ“Œ Task Objectives

Load a pre-trained GPT-2 model using Hugging Face.
Prepare a small, domain-specific dataset.
Fine-tune GPT-2 on the dataset.
Generate text based on a given prompt using the fine-tuned model.


ðŸ“ˆ Results

Sample output generated by the fine-tuned GPT-2 model:
Prompt: Generative AI is
Output: Generative AI is transforming industries by enabling machines to generate content such as text, images, and more with human-like quality.

ðŸš€ How to Run This Notebook

1. Open the notebook in Google Colab.
2. Run each cell in order.
3. Replace the dataset in train.txt with your own text for customization.
4. Modify the prompt in the generation cell for different results.
